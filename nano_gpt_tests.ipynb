{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import nano_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = f.read()\n",
    "\n",
    "# The properties of the test\n",
    "chars = sorted(list(set(corpus)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Encoding and decoding\n",
    "string_to_int = {character: index for index, character in enumerate(chars)}\n",
    "int_to_string = {index: character for index, character in enumerate(chars)}\n",
    "encode = lambda string: [string_to_int[char] for char in string]\n",
    "decode = lambda list_int: \"\".join([int_to_string[integer] for integer in list_int])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(corpus), dtype=torch.long, device=device)\n",
    "number_train = int(0.9 * len(data))\n",
    "train_data = data[:number_train]\n",
    "validation_data = data[:number_train]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can set up the model hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "block_size = 256\n",
    "max_iters = 10000\n",
    "eval_interval = 100\n",
    "learning_rate = 3e-4\n",
    "dropout = 0.2\n",
    "eval_iters = 100\n",
    "number_layers = 6\n",
    "number_heads = 6\n",
    "number_embeddings = 384  # 384 / 6 = 64 dimensional heads"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nano_gpt.BigramLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    number_embeddings=number_embeddings,\n",
    "    block_size=block_size,\n",
    "    number_heads=number_heads,\n",
    "    number_layers=number_layers,\n",
    "    dropout=dropout,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step 0: train loss 4.3349, val loss 4.3343.\n",
      "At step 100: train loss 2.4786, val loss 2.4773.\n",
      "Training manually interrupted.\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "model.train_model(\n",
    "    max_iters,\n",
    "    train_data,\n",
    "    validation_data,\n",
    "    optimizer,\n",
    "    batch_size,\n",
    "    eval_interval,\n",
    "    eval_iters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"shakespeare.torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"shakespeare.torch\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the final validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.3950, val loss 0.3948.\n"
     ]
    }
   ],
   "source": [
    "losses = nano_gpt.estimate_loss(model, 200, block_size, batch_size, train_data, validation_data)\n",
    "print(f\"Train loss {losses['train']:.4f}, val loss {losses['val']:.4f}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an example of the produced text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An example of text generated from the model.\n",
      "\n",
      "end love's dooms! Sweet thou help!\n",
      "O prince! thou dost uttake before I have loved thee,\n",
      "But I with my age for an Edward's grave.\n",
      "\n",
      "CATESBY:\n",
      "Slandering and deliver to unfly the rage.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "My gracious urge is chair, I thank thee;\n",
      "I will return to her at the pride;\n",
      "And, if I die, to fill \n"
     ]
    }
   ],
   "source": [
    "# Generate from the model\n",
    "print(\"\\nAn example of text generated from the model.\")\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, 300)[0].tolist()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also have the model generate text forever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Volsces\n",
      "Hath greateful ladies.\n",
      "\n",
      "ARCHIDAMUS:\n",
      "Iffer the king must die their own perforce.\n",
      "\n",
      "MONTAGUE:\n",
      "But he's a man to do so, my lord.\n",
      "\n",
      "MARCIUS:\n",
      "As the was always poor infection, as they say\n",
      "To the Marcius shall be honour to the people!\n",
      "\n",
      "All:\n",
      "Content that so my brother,\n",
      "The mayor to the play with my tongue.\n",
      "Cousin of Buckingham, my my brother wrongs:\n",
      "But look to the earth, I am gall'd unto my face.\n",
      "But, moreo, my good life, my crown, and fear.\n",
      "Nothing come into his honour in your business,\n",
      "The which he had mived your father forfeit,\n",
      "Both with his condition with youth air, 'Will't not be;\n",
      "From so your judgment act an our house,\n",
      "And so harms come to 't.\n",
      "\n",
      "POMPEY:\n",
      "True, to the bloody battle, the frant, of them are this\n",
      "To give your great errancon's sense; and these two beats\n",
      "As he what in them, like an empty helm\n",
      "With touch on Paris'ies. Lord Afort night,\n",
      "Bloody though is hot befell may disposition\n",
      "The narror of England's war, and more heart.\n",
      "But, soft! where is this?\n",
      "\n",
      "First Citizen:\n",
      "Give me to prisoners; extremity\n",
      "the good presence to the liquit throngs with\n",
      "oath! To see how I wanton what the cause of you!\n",
      "\n",
      "CORIOLANUS:\n",
      "My has mindred heart's love's retired by the Antipodes\n",
      "Slaughter of Juliet?\n",
      "My duty's son, hearing of our foes\r"
     ]
    }
   ],
   "source": [
    "model.generate_forever(decode, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amlengine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Jan 11 2023, 15:21:40) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aef9ab2bc9beef17b1faf086a773f3268916132bada9abfc1a3d0efa2022ed27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
